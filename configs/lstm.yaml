#lstm
data_settings:
  dataset_path: "dataset/merged_pairs.json"
  glove_path: "glove/glove.6B.100d.txt"
  vocab_path: "dataset/vocab.pkl"
  min_freq: 2
  max_len: 50
  split_ratios: [0.8, 0.1, 0.1]

model_settings:
  rnn_type: "lstm"
  emb_dim: 100
  hidden_dim: 256
  n_layers: 2
  dropout: 0.5
  pad_idx: 0
  sos_idx: 1
  eos_idx: 2

train_settings:
  batch_size: 64
  num_epochs: 20
  learning_rate: 0.001
  teacher_forcing_ratio: 0.5
  clip_grad: 1.0
  num_workers: 2

experiment_settings:
  experiment_name: "seq2seq_lstm_baseline"
  project: "open_domain_chatbot"
  use_wandb: true

